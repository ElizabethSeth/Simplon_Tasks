{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElizabethSeth/Simplon_tasks/blob/Plateforme_docker/intro_SparkSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NwIryDy-jfo"
      },
      "source": [
        "## DataFrames et Spark SQL\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmDTlsPjAQPK",
        "outputId": "31d4fb99-45b1-4c42-edce-603a7b1a9dcf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=2930b20e1b6155f017247e8b563f8c79ac32b395dd4d5f52e25cfd8374a130a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "p-xM_TeG-jfr",
        "outputId": "70e2aaf3-789c-4a12-e134-f26c36cd0d47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=Spark SQL>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c3b00e75b22a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark SQL</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# create entry points to spark\n",
        "try:\n",
        "    sc.stop()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Spark SQL\") \\\n",
        "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEezqyB6-jfs"
      },
      "source": [
        "### Generate your own DataFrame\n",
        "Instead of accessing the file system, let's create a DataFrame by generating the data.  In this case, we'll first create the `stringRDD` RDD and then convert it into a DataFrame when we're reading `stringJSONRDD` using `spark.read.json`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EM7MRsGM-jfs"
      },
      "outputs": [],
      "source": [
        "# Generate our own JSON data\n",
        "\n",
        "stringJSONRDD = sc.parallelize((\"\"\"\n",
        "  { \"id\": \"123\",\n",
        "    \"name\": \"Katie\",\n",
        "    \"age\": 19,\n",
        "    \"eyeColor\": \"brown\"\n",
        "  }\"\"\",\n",
        "   \"\"\"{\n",
        "    \"id\": \"234\",\n",
        "    \"name\": \"Michael\",\n",
        "    \"age\": 22,\n",
        "    \"eyeColor\": \"green\"\n",
        "  }\"\"\",\n",
        "  \"\"\"{\n",
        "    \"id\": \"345\",\n",
        "    \"name\": \"Simone\",\n",
        "    \"age\": 23,\n",
        "    \"eyeColor\": \"blue\"\n",
        "  }\"\"\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2CVppx2t-jft"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame\n",
        "swimmersJSON = spark.read.json(stringJSONRDD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5fJS-ji-jft",
        "outputId": "f6e90d47-f313-4cf4-ba44-420dbe6a44df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[age: bigint, eyeColor: string, id: string, name: string]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "swimmersJSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQomeLT1-jft",
        "outputId": "ed0b7892-09e7-4404-acc9-99a2aba69f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+---+-------+\n",
            "|age|eyeColor| id|   name|\n",
            "+---+--------+---+-------+\n",
            "| 19|   brown|123|  Katie|\n",
            "| 22|   green|234|Michael|\n",
            "| 23|    blue|345| Simone|\n",
            "+---+--------+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "swimmersJSON.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK0jdZ0w-jft"
      },
      "source": [
        "DataFrame.createOrReplaceTempView(name: str) → None\n",
        "\n",
        "Creates or replaces a local temporary view with this DataFrame.\n",
        "\n",
        "=> The lifetime of this temporary table is tied to the SparkSession that was used to create this DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4nxln9j_-jfu"
      },
      "outputs": [],
      "source": [
        "# Create temporary table\n",
        "\n",
        "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-ovjdkk-jfu",
        "outputId": "04d2b5dc-9c2b-477b-e66d-c8ca016225bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+---+-------+\n",
            "|age|eyeColor| id|   name|\n",
            "+---+--------+---+-------+\n",
            "| 19|   brown|123|  Katie|\n",
            "| 22|   green|234|Michael|\n",
            "| 23|    blue|345| Simone|\n",
            "+---+--------+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# DataFrame API\n",
        "swimmersJSON.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzR3Nvhs-jfu",
        "outputId": "d3893f7c-680f-4a04-db58-fb3a8a9f0f47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
              " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
              " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# SQL Query\n",
        "spark.sql(\"select * from swimmersJSON\").collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXRTl3A4-jfu"
      },
      "source": [
        "#### Inferring the Schema Using Reflection\n",
        "Note that Apache Spark is inferring the schema using reflection; i.e. it automaticlaly determines the schema of the data based on reviewing the JSON data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z0FKy5q-jfv",
        "outputId": "2a7a409b-52f5-4f02-df74-14cf37b8eb42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- eyeColor: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the schema\n",
        "swimmersJSON.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2TDqlep-jfv"
      },
      "source": [
        "Notice that Spark was able to determine infer the schema (when reviewing the schema using `.printSchema`).\n",
        "\n",
        "But what if we want to programmatically specify the schema?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQkUv9Rw-jfv"
      },
      "source": [
        "#### Programmatically Specifying the Schema\n",
        "In this case, let's specify the schema for a `CSV` text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5S5Sd95-jfv"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# Generate our own CSV data\n",
        "#   This way we don't have to access the file system yet.\n",
        "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
        "\n",
        "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
        "schemaString = \"id name age eyeColor\"\n",
        "schema = StructType([\n",
        "    StructField(\"id\", LongType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", LongType(), True),\n",
        "    StructField(\"eyeColor\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Apply the schema to the RDD and Create DataFrame\n",
        "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
        "\n",
        "# Creates a temporary view using the DataFrame\n",
        "swimmers.createOrReplaceTempView(\"swimmers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhW9edG4-jfv",
        "outputId": "f5f4679e-390f-4fc5-a7ac-b97d03591864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- eyeColor: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the schema\n",
        "#   Notice that we have redefined id as Long (instead of String)\n",
        "swimmers.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIidBauw-jfv",
        "outputId": "bda006f4-4473-4673-8ea0-e0953bd7db2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(id=123, name='Katie', age=19, eyeColor='brown'),\n",
              " Row(id=234, name='Michael', age=22, eyeColor='green'),\n",
              " Row(id=345, name='Simone', age=23, eyeColor='blue')]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# SQL Query\n",
        "spark.sql(\"select * from swimmers\").collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSzNAHkK-jfv"
      },
      "source": [
        "As you can see from above, we can programmatically apply the `schema` instead of allowing the Spark engine to infer the schema via reflection.\n",
        "\n",
        "Additional Resources include:\n",
        "* [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n",
        "* [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema): This is in reference to Programmatically Specifying the Schema using a `CSV` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1iEFd97-jfw"
      },
      "source": [
        "#### SparkSession\n",
        "\n",
        "Notice that we're no longer using `sqlContext.read...` but instead `spark.read...`.  This is because as part of Spark 2.0, `HiveContext`, `SQLContext`, `StreamingContext`, `SparkContext` have been merged together into the Spark Session `spark`.\n",
        "* Entry point for reading data\n",
        "* Working with metadata\n",
        "* Configuration\n",
        "* Cluster resource management\n",
        "\n",
        "For more information, please refer to [How to use SparkSession in Apache Spark](https://sparkbyexamples.com/spark/sparksession-explained-with-examples/) ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYU5Luk_-jfw"
      },
      "source": [
        "### Querying with the DataFrame API\n",
        "With DataFrames, you can start writing your queries using the DataFrame API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r0_R360-jfw",
        "outputId": "a4f9d5bf-153a-47e3-fb0a-c341dc23b966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+---+--------+\n",
            "| id|   name|age|eyeColor|\n",
            "+---+-------+---+--------+\n",
            "|123|  Katie| 19|   brown|\n",
            "|234|Michael| 22|   green|\n",
            "|345| Simone| 23|    blue|\n",
            "+---+-------+---+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show the values\n",
        "swimmers.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWb2ZUzp-jfw",
        "outputId": "75ae2535-6b19-45c3-e0ff-fc66f7878ae2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string, age: bigint, eyeColor: string]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Using Databricks `display` command to view the data easier\n",
        "display(swimmers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzGPtM-4-jfw",
        "outputId": "9733c01b-4724-4914-b4f8-f069c1747cd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get count of rows\n",
        "swimmers.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCShaM_Q-jfw",
        "outputId": "95a82b38-e43b-45c9-c893-0f3ba213d919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the id, age where age = 22\n",
        "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwawzy2I-jfw",
        "outputId": "3fc3cf40-6b5e-4028-a858-f36672135212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------+\n",
            "|  name|eyeColor|\n",
            "+------+--------+\n",
            "| Katie|   brown|\n",
            "|Simone|    blue|\n",
            "+------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get the name, eyeColor where eyeColor like 'b%'\n",
        "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boLy8twy-jfw"
      },
      "source": [
        "### Querying with SQL\n",
        "With DataFrames, you can start writing your queries using `Spark SQL` - a SQL dialect that is compatible with the Hive Query Language (or HiveQL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s57YXmzV-jfw",
        "outputId": "c878f674-a4b6-492a-a235-aa1a82e2cacf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+---+--------+\n",
            "| id|   name|age|eyeColor|\n",
            "+---+-------+---+--------+\n",
            "|123|  Katie| 19|   brown|\n",
            "|234|Michael| 22|   green|\n",
            "|345| Simone| 23|    blue|\n",
            "+---+-------+---+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Execute SQL Query and return the data\n",
        "spark.sql(\"select * from swimmers\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXtIFRhV-jfw"
      },
      "source": [
        "Let's get the row count:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jdMV8Vi-jfx",
        "outputId": "ca4f1cc7-81e0-41d8-b3da-28f355f6a264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       3|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get count of rows in SQL\n",
        "spark.sql(\"select count(1) from swimmers\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9tdkGlz-jfx",
        "outputId": "65915153-0cee-44e7-96a0-7017694312b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query id and age for swimmers with age = 22 via DataFrame API\n",
        "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oZQX7HT-jfx",
        "outputId": "b7f90a31-cc23-4dc6-e000-ed058a3c68e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query id and age for swimmers with age = 22 via DataFrame API in another way\n",
        "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4isrSU6J-jfx",
        "outputId": "122e1411-de04-4b59-d122-852d118ca1c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query id and age for swimmers with age = 22 in SQL\n",
        "spark.sql(\"select id, age from swimmers where age = 22\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hK9kNeK-jfx",
        "outputId": "8520c101-4203-4fd0-800f-a144333c4c11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------+\n",
            "|  name|eyeColor|\n",
            "+------+--------+\n",
            "| Katie|   brown|\n",
            "|Simone|    blue|\n",
            "+------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
        "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNrCNT8K-jfx"
      },
      "source": [
        "## Application:\n",
        "\n",
        "Query flight departure delays by State and City by joining the departure delay and join to the airport codes (to identify state and city)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiKyqaSa-jfx"
      },
      "source": [
        "* On-Time Performance Datasets\n",
        "\n",
        "The source `airports` dataset can be found at [OpenFlights Airport, airline and route data](https://openflights.org/data.php).\n",
        "\n",
        "The `flights`, also known as the `departuredelays`, dataset can be found at [Airline On-Time Performance and Causes of Flight Delays: On_Time Data](https://catalog.data.gov/dataset/airline-on-time-performance-and-causes-of-flight-delays-on-time-data)\n",
        "\n",
        "1- Read into spark DataFrames the datasets departuredelays.csv and airport-codes.txt.\n",
        "\n",
        "2- display dataframe with .show(), .cache() , print the data schema\n",
        "\n",
        "3- Create a local temporary view with these dataframes.\n",
        "\n",
        "4- answer the queries below:\n",
        "\n",
        "* Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
        "* Query Sum of Flight Delays by State (for the US)\n",
        "* Add 2 more analysis axes of your choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSJhN8Nl-jfx",
        "outputId": "b2c342ce-4465-4a40-c01e-1ef2d87f923e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+--------+------+-----------+\n",
            "|    date|delay|distance|origin|destination|\n",
            "+--------+-----+--------+------+-----------+\n",
            "|01011245|    6|     602|   ABE|        ATL|\n",
            "|01020600|   -8|     369|   ABE|        DTW|\n",
            "|01021245|   -2|     602|   ABE|        ATL|\n",
            "|01020605|   -4|     602|   ABE|        ATL|\n",
            "|01031245|   -4|     602|   ABE|        ATL|\n",
            "|01030605|    0|     602|   ABE|        ATL|\n",
            "|01041243|   10|     602|   ABE|        ATL|\n",
            "|01040605|   28|     602|   ABE|        ATL|\n",
            "|01051245|   88|     602|   ABE|        ATL|\n",
            "|01050605|    9|     602|   ABE|        ATL|\n",
            "|01061215|   -6|     602|   ABE|        ATL|\n",
            "|01061725|   69|     602|   ABE|        ATL|\n",
            "|01061230|    0|     369|   ABE|        DTW|\n",
            "|01060625|   -3|     602|   ABE|        ATL|\n",
            "|01070600|    0|     369|   ABE|        DTW|\n",
            "|01071725|    0|     602|   ABE|        ATL|\n",
            "|01071230|    0|     369|   ABE|        DTW|\n",
            "|01070625|    0|     602|   ABE|        ATL|\n",
            "|01071219|    0|     569|   ABE|        ORD|\n",
            "|01080600|    0|     369|   ABE|        DTW|\n",
            "+--------+-----+--------+------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------+-----+-------+----+\n",
            "|       City|State|Country|IATA|\n",
            "+-----------+-----+-------+----+\n",
            "| Abbotsford|   BC| Canada| YXX|\n",
            "|   Aberdeen|   SD|    USA| ABR|\n",
            "|    Abilene|   TX|    USA| ABI|\n",
            "|      Akron|   OH|    USA| CAK|\n",
            "|    Alamosa|   CO|    USA| ALS|\n",
            "|     Albany|   GA|    USA| ABY|\n",
            "|     Albany|   NY|    USA| ALB|\n",
            "|Albuquerque|   NM|    USA| ABQ|\n",
            "| Alexandria|   LA|    USA| AEX|\n",
            "|  Allentown|   PA|    USA| ABE|\n",
            "|   Alliance|   NE|    USA| AIA|\n",
            "|     Alpena|   MI|    USA| APN|\n",
            "|    Altoona|   PA|    USA| AOO|\n",
            "|   Amarillo|   TX|    USA| AMA|\n",
            "|Anahim Lake|   BC| Canada| YAA|\n",
            "|  Anchorage|   AK|    USA| ANC|\n",
            "|   Appleton|   WI|    USA| ATW|\n",
            "|     Arviat|  NWT| Canada| YEK|\n",
            "|  Asheville|   NC|    USA| AVL|\n",
            "|      Aspen|   CO|    USA| ASE|\n",
            "+-----------+-----+-------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TBD\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Spark_Departure_flights\") \\\n",
        "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sc\n",
        "\n",
        "departures = \"departuredelays.csv\"\n",
        "airoports = \"airport-codes.txt\"\n",
        "\n",
        "depart = spark.read.csv(departures, header=True)\n",
        "airport_codes = spark.read.csv(airoports, header=True, sep=\"\\t\")\n",
        "\n",
        "depart.createOrReplaceTempView(\"departures\")\n",
        "airport_codes.createOrReplaceTempView(\"airports\")\n",
        "\n",
        "depart.show()\n",
        "airport_codes.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "depart.printSchema()\n",
        "airport_codes.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZagN87lGN4S",
        "outputId": "b341da0d-29d3-48d5-cf73-64295b5d2d74"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: string (nullable = true)\n",
            " |-- delay: string (nullable = true)\n",
            " |-- distance: string (nullable = true)\n",
            " |-- origin: string (nullable = true)\n",
            " |-- destination: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- IATA: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0UZ5g4L-jfx",
        "outputId": "ef1848d8-1d72-4332-c8b9-b590944b43ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "depart.cache()\n",
        "#airport_codes.cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "airport_codes.select(\"City\", \"State\").filter(airport_codes.State == \"NM\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHK2HRYjWSTm",
        "outputId": "b9e381e1-569f-455d-abee-793feeead954"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|       City|State|\n",
            "+-----------+-----+\n",
            "|Albuquerque|   NM|\n",
            "|   Carlsbad|   NM|\n",
            "|     Clovis|   NM|\n",
            "| Farmington|   NM|\n",
            "|      Hobbs|   NM|\n",
            "| Las Cruces|   NM|\n",
            "|    Roswell|   NM|\n",
            "|   Santa Fe|   NM|\n",
            "|Silver City|   NM|\n",
            "+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "depart.select(\"origin\", \"delay\").filter(depart.origin == \"ABE\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hHRQqvKHWoZ",
        "outputId": "17750248-95c8-42ae-d907-ffba81f47cc9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|origin|delay|\n",
            "+------+-----+\n",
            "|   ABE|    6|\n",
            "|   ABE|   -8|\n",
            "|   ABE|   -2|\n",
            "|   ABE|   -4|\n",
            "|   ABE|   -4|\n",
            "|   ABE|    0|\n",
            "|   ABE|   10|\n",
            "|   ABE|   28|\n",
            "|   ABE|   88|\n",
            "|   ABE|    9|\n",
            "|   ABE|   -6|\n",
            "|   ABE|   69|\n",
            "|   ABE|    0|\n",
            "|   ABE|   -3|\n",
            "|   ABE|    0|\n",
            "|   ABE|    0|\n",
            "|   ABE|    0|\n",
            "|   ABE|    0|\n",
            "|   ABE|    0|\n",
            "|   ABE|    0|\n",
            "+------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"SELECT origin, ROUND(AVG(delay), 2) AS avg_delay\n",
        "FROM departures\n",
        "GROUP BY origin\n",
        "ORDER BY avg_delay DESC\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFERfWUKHWtp",
        "outputId": "eab4560b-3623-4291-d921-7051791c72e2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+\n",
            "|origin|avg_delay|\n",
            "+------+---------+\n",
            "|   GUM|    33.88|\n",
            "|   LSE|    26.53|\n",
            "|   MQT|    23.87|\n",
            "|   EGE|    20.57|\n",
            "|   ROA|    19.89|\n",
            "|   MDW|    19.66|\n",
            "|   BTV|    18.72|\n",
            "|   ORD|    18.59|\n",
            "|   IAD|     18.4|\n",
            "|   SCE|    17.92|\n",
            "|   GUC|    17.73|\n",
            "|   ISP|     17.7|\n",
            "|   ALO|    17.16|\n",
            "|   LNK|    17.02|\n",
            "|   DEN|    16.92|\n",
            "|   BWI|    16.83|\n",
            "|   CID|    16.58|\n",
            "|   PBI|    16.56|\n",
            "|   FLL|    16.51|\n",
            "|   JFK|    16.46|\n",
            "+------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = spark.sql(\"\"\"\n",
        "SELECT a.City, d.origin, SUM(d.delay) AS total_delay\n",
        "FROM departures d\n",
        "JOIN airports a ON d.origin = a.IATA\n",
        "WHERE a.State = 'WA'\n",
        "GROUP BY a.City, d.origin\n",
        "ORDER BY total_delay DESC\n",
        "\"\"\")\n",
        "\n",
        "result_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9uizIR_LZQp",
        "outputId": "61da5e44-f331-4f34-9ae0-d0f68ab822f1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- City: string (nullable = true)\n",
            " |-- origin: string (nullable = true)\n",
            " |-- total_delay: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT a.City, d.origin, SUM(d.delay) AS total_delay\n",
        "FROM departures d\n",
        "JOIN airports a ON d.origin = a.IATA\n",
        "WHERE a.State = 'WA'\n",
        "GROUP BY a.City, d.origin\n",
        "ORDER BY total_delay DESC\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AptxFDuaMQK-",
        "outputId": "5ffa9ae9-651a-4c95-c68e-b95356829e4e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------+\n",
            "|   City|origin|total_delay|\n",
            "+-------+------+-----------+\n",
            "|Seattle|   SEA|   159086.0|\n",
            "|Spokane|   GEG|    12404.0|\n",
            "|  Pasco|   PSC|      949.0|\n",
            "+-------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnhqcYeN-jf1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWHCJHpz-jf1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTMPKEN7-jf1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMju106Y-jf1"
      },
      "source": [
        "# Useful Ressources:\n",
        "\n",
        "For more information, please refer to:\n",
        "* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)\n",
        "* [PySpark SQL Module: DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.html)\n",
        "* [PySpark SQL Functions Module](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX-ftone-jf1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "name": "Ch4 - DataFrames",
    "notebookId": 4341522646494009,
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}